#  Gesture Vocalizer: A Vision-Based Assistive Communication System

**Project by:** Syeda Umaima Tamkeen

---

## ğŸ“ Summary

The **Gesture Vocalizer** is a real-time, vision-based assistive communication system that converts **hand gestures** into **spoken language**. Designed for individuals with speech and hearing impairments, this system uses **computer vision** and **machine learning** to detect, classify, and vocalize static hand gestures using webcam input.

This project demonstrates how artificial intelligence and deep learning can bridge communication gaps for differently-abled communities, supporting Sustainable Development Goals (SDGs) for **inclusion** and **accessibility**.

---

## ğŸ” Features

- ğŸ–ï¸ Real-time hand gesture detection using **MediaPipe**
- ğŸ§  Gesture classification using **XGBoost** and **CNN**
- ğŸ“Š Preprocessed dataset with landmarks and image features
- ğŸ”Š Text-to-speech conversion using **pyttsx3**
- ğŸ“ˆ Visualizations and performance analytics
- ğŸŒ Streamlit GUI for user interaction

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ app.py # Main Streamlit interface
â”œâ”€â”€ create_dataset.py # Data collection script
â”œâ”€â”€ model.pkl / model_lp # Trained model files
â”œâ”€â”€ visualizations/ # Charts, graphs, and heatmaps
â”œâ”€â”€ xgboost_output/ # Output from XGBoost classifier
â”œâ”€â”€ models/ # Model architecture/code
â”œâ”€â”€ .idea/, .venv/ # VSCode & environment folders
â”œâ”€â”€ *.py, *.csv, *.png, *.pickle # Supporting project files
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file


---

##  How It Works

1. **Hand Landmark Detection**  
   Uses **MediaPipe** to detect 21 key hand landmarks from the webcam feed or image.

2. **Feature Extraction**  
   Extracts angles/distances or image features to represent each gesture numerically.

3. **Model Prediction**  
   Trained models like **XGBoost**, **Random Forest**, or **CNN** classify gestures in real-time.

4. **Voice Output**  
   The recognized label is converted into audio using **pyttsx3**.

---

##  How to Run

###  Installation
```bash
git clone https://github.com/syeda-umaima-tamkeen/Gesture-Vocalizer-Assistive-Communication.git
cd Gesture-Vocalizer-Assistive-Communication
pip install -r requirements.txt

## â–¶ï¸ Run the App
streamlit run app.py

ğŸ“Š Results
âœ… High accuracy on 35 custom gesture classes

ğŸ¯ Real-time prediction speed with low latency

ğŸ“‰ Loss and accuracy graphs show strong model convergence

ğŸ“Š Heatmaps and PCA plots demonstrate clear gesture separation

ğŸ“Œ Applications
ğŸ”‡ Assistive technology for the speech and hearing impaired

ğŸ§  Human-computer interaction

ğŸ¤– Smart homes or gesture-controlled devices

ğŸ« Educational tools for sign language learning

ğŸ› ï¸ Tech Stack
Python 3.9+

MediaPipe â€“ hand landmark detection

XGBoost / CNN â€“ gesture classification

Pyttsx3 â€“ text-to-speech engine

Streamlit â€“ web app deployment

Matplotlib / Seaborn â€“ visualization

Pickle â€“ model serialization

ğŸ License
This project is licensed under the MIT License 

